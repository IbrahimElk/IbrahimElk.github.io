---
layout: post
title: "Monte Carlo, Temporal Difference, and Deep Learning Approaches in Goal-Seeking Tasks"
date: 2025-06-01
summary: "Making agents less stupid. A hands-on exploration of reinforcement learning that compares classical tabular methods with modern deep learning approaches"
keywords:
  ["RL", "DRL", "q-learning", "sarsa", "PPO", "DQN", "hydra", "gym", "stable-baseline3"]
categories: projects
---

**Author:** Ibrahim El Kaddouri  

**Repository:** <span class="tooltip">
<a href="https://github.com/IbrahimElk/agents-with-issues">
    <img src="/assets/images//2025-06-01/environments/elf_down.png">
</a>
<span class="tooltip-text"> The repository is still private, under construction </span>
</span>

## Introduction

In this report, we will explore the implementation and analysis of
various Reinforcement Learning algorithms within a grid-based game
environment. The report begins by explaining some important concepts of
the RL algorithms, proceeded by some hyperparameter experimentation and
concluding with an examination of Deep Reinforcement Learning
approaches.

## 1. Environments and Baselines

### 1.1 Custom Environment and Random Baseline

This portion focuses on creating a custom environment alongside creating
a random agent as baseline.

#### goal-seeking environment design

We will implement a goal finding environment where an agent tries to
find the goal in a variable size (*n* × *m*) grid-world. The game starts
with the player at location (0, 0) of the (*n* × *m*) grid world with
the goal (’cookie’) located at (*n* − 1, *m* − 1). The player makes
moves until they reach the goal. There are four actions possible: up,
down, right and left. In terms of the reward structure, each movement
incurs a penalty of -1 points, while reaching the target state yields a
reward of 0 points. The environment architecture follows the OpenAI Gym
[OpenAI Gym](https://gymnasium.farama.org/api/env/) framework, by
implementing the abstract methods of the class `gym.Env`, i.e.
`reset()`, `step()` and `render()`.  
The state of the agent in this particular environment can be represented
by a single number. The environment doesn’t change, thus the agent’s
current coordinates provide sufficient state information. There are
*n* × *m* − 1 possible states. The player cannot be at the goal position
as that results in the end of the episode.  
Environmental observations are provided to the agent following each
action. These observations are delivered through both `step` and `reset`
function calls, representing the agent’s current location calculated
using the formula:
index = current_row × ncols + current_col
For example, the starting position can be calculated as follows:
0 \* *m* + 0 = 0. The complete implementation of the `cookie`
environment can be found in the following file: `src/envs/cookie.py`.

#### random agent baseline

An agent in any kind of environment needs to be able to do two things:
first, perform an action within a specific environment and second, learn
from previous interactions. Thus, any agent implemented in this report
will have the following two abstract methods implemented: `act()` and
`learn()`.  
We will begin with a baseline implementation: the random agent. A random
agent selects an action uniformly at random, i.e., according to a
uniform distribution. The complete implementation of the `random` agent
can be found in the following file: `src/baseline/random.py`.

#### performance evaluation

There are different ways to compare the performance of agents. One
common approach is to compare their mean average reward obtained during
training or during inference.  
Following each environmental step, the agent receives a reward. The
average of all rewards within a single episode is computed and added to
a tracking list. This reward sequence is plotted against training
episodes, with a moving average applied to smooth the curves and reduce
variability caused by exploratory randomness. The implementation of the
`training` procedure can be found in the following file:
`src/tasks/training.py`.

#### evolution of random agent

Using a 5 × 5 instance of the `cookie` environment, we will plot the
evolution of the average return over an interaction of 10000 episodes by
the random agent. After that, we visualize using a GIF the steps of an
episode of the random agent.


<div class="image-grid" id="fig:environments">
  <img src="/assets/images/2025-06-01/task1_1/moving_average_return.png"/>
  <img src="/assets/images/2025-06-01/task1_1/number_of_steps.png" />
</div>

<div style="text-align: center;">
  <img src="/assets/images/2025-06-01/task1_1/episode.gif"
       style="width: 50%; height: auto;"> 
  <figcaption>All episode steps in the cookie environment</figcaption>
</div>


### 1.2 Minihack environment and fixed baseline

Consider the four Minihack environments in Figure
<a href="#fig:environments" data-reference-type="ref"
data-reference="fig:environments">3</a>. Movement is restricted to
cardinal directions (north, south, east, west), matching the previously
described cookie environment’s action constraints.

- `EMPTY_ROOM` is a simple goal-finding-like environment, where the goal
  is to reach the downstairs. The agent gets −1 reward for each step.

- `CLIFF` is based on the Cliff-environment of Sutton and Barto book
  (Chapter 6). The reward scheme is: 0 when reaching the goal, −1 for
  each step. Stepping on the lava gives −100 reward and teleports the
  agent to the initial state, without resetting the episode. The episode
  only ends when the goal is reached.

- `ROOM_WITH_LAVA` is a slightly more complicated goal-finding
  environment. The reward scheme is the same as the `CLIFF`.

- `ROOM_WITH_MONSTER` is an environment identical to the `EMPTY_ROOM`,
  but a monster walks around and can attack and kill the agent. The
  reward scheme is the same as the `CLIFF` environment and −100 for any
  death.

<div class="image-grid" id="fig:environments">
  <img src="/assets/images/2025-06-01/environments/empty_room.png">
  <img src="/assets/images/2025-06-01/environments/room_with_monster.png">
  <img src="/assets/images/2025-06-01/environments/cliff.png">
  <img src="/assets/images/2025-06-01/environments/room_with_lava.png">
</div>
<figcaption>The four MiniHack environments</figcaption>


#### fixed agent baseline

We will construct a new baseline: the fixed agent for these four
environments. A fixed agent always moves down until it cannot move any
further, and then always moves right. Unlike the random agent (which
acts independently of its position), the fixed agent must take into
account its location in the environment. Therefore, we need a
representation of the agent’s position and surroundings.  
Note that we cannot reuse the same state representation as in the
`cookie` environment, since one of our four environments is
non‐stationary. In a non‐stationary environment, the agent might be in
the same *coordinate* position but a different *state*, because other
objects have moved and may influence the agent’s next action.  
For example, in the `ROOM_WITH_MONSTER` environment, one could encode
the state as a sequence of coordinates: each pair would represent the
location of a moving object (either the agent or a monster). Instead, we
choose a much simpler (though less compact) representation: the
ASCII‐based[^1] NetHack view. The implementation of the `fixed agent`
can be found in the following file: `src/baseline/fixed.py`.

#### evolution of fixed agent

In Figure <a href="#fig:fixed-sidebyside" data-reference-type="ref"
data-reference="fig:fixed-sidebyside">4</a>, we visualize using a GIF
the steps of an episode of the fixed agent on the
`EMPTY_ROOM` and `ROOM_WITH_LAVA` environments.

<div class="image-grid" id="fig:fixed-sidebyside">
  <img src="/assets/images/2025-06-01/task1_2/fixed_empty_room/episode.gif"
       style="width: auto; height: 50%;"/>
  <img src="/assets/images/2025-06-01/task1_2/fixed_room_with_lava/episode.gif" />
</div>

## 2. Learning Algorithm Implementation and Analysis

### 2.1 Algorithmic Experimentation

This portion focuses on implementing various reinforcement learning
algorithms and conducting comparative analysis across different
environments. Three distinct learning agents have been developed:

- Monte Carlo On-policy

- Temporal Difference On-policy (Sarsa)

- Temporal Difference Off-policy (Q-learning)

All agents utilize *ϵ*-greedy exploration strategies during the learning
phase. To enhance exploratory behaviour during training, exploring
starts are incorporated into the process. The complete implementation of
these three agents is located in: `src/algorithms`.

<span id="sec:on_policy_vs_off_policy"
label="sec:on_policy_vs_off_policy"></span>

#### on-policy versus off-policy

This analysis begins by examining the performance differences between
on-policy and off-policy approaches. Brief review: A reinforcement
learning algorithm is classified as on-policy when the action selected
for Q-value updates matches the action chosen during execution.
Conversely, off-policy algorithms maintain distinct target and behaviour
policies.  
The comparison utilizes the `CLIFF` environment to evaluate Sarsa and
Q-learning algorithms with *ϵ*-greedy action selection (*ϵ* = 0.2). A
constant learning rate of (*α* = 0.3) is maintained throughout this
experimentation (complete configuration details available in
`src/tasks/config/task2_1.yaml`). It should be noted that evaluation
occurs deterministically with (*ϵ* = 0) for all agents unless stated
otherwise.  
The results in Figure <a href="#fig:task1_2-0" data-reference-type="ref"
data-reference="fig:task1_2-0">5</a> demonstrate that while Q-learning
discovers optimal policy values, its online performance falls short of
Sarsa, which develops a safer, indirect route strategy. Both methods
would eventually converge to optimal policies if *ϵ* were progressively
decreased .

<div class="image-grid" id="fig:task1_2-0">
  <img src="/assets/images/2025-06-01/task2_1/on_policy_vs_off_policy/moving_avg_return.png"/>
  <img src="/assets/images/2025-06-01/task2_1/on_policy_vs_off_policy/num_steps_per_episode.png"/>
</div>
<div style="text-align: center;">
<figcaption>cliff-walking environment, the results are from a single run, but smoothed</figcaption>
</div>
<br>

Q-learning develops values for the optimal strategy, involving choosing
actions along the cliff edge. However, this approach occasionally
results in cliff falls due to *ϵ*-greedy action selection. Sarsa takes
action selection into account and learns a longer but safer pathway
through the upper grid region .

<div class="image-grid" id="fig:task1_2-1">
  <img src="/assets/images/2025-06-01/task2_1/on_policy_vs_off_policy/qlearning/policy.png"/>
  <img src="/assets/images/2025-06-01/task2_1/on_policy_vs_off_policy/sarsa/policy.png"/>
</div>
<div style="text-align: center;">
<figcaption>Left figure shows policy of Q-Learning and to the right, that of Sarsa</figcaption>
</div>

#### monte carlo versus temporal difference analysis

Let’s start with a brief overview of first-visit Monte Carlo: A
state-action pair (*s*, *a*) is considered visited during an episode
when state *s* is encountered and action *a* is executed. The
every-visit MC approach estimates state-action pair values by averaging
returns following all visits. The first-visit MC method averages returns
following the initial visit in each episode .  
Advantageous Monte Carlo scenarios are demonstrated in the
`ROOM_WITH_MONSTER` environment, while challenging scenarios appear in
the `CLIFF` environment due to environment size and insufficient
training episodes. The Monte Carlo algorithm converges in
`ROOM_WITH_MONSTER` to a mean average return value of −0.5, whilst for
sarsa, it converges to a value of −0.92.

<div class="image-row" id="fig:task2_1-3">
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/good_monster/moving_avg_return.png"/>
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/good_monster/num_steps.png"/>
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/good_monster/num_steps_per_episode.png"/>
</div>
<div style="text-align: center;">
<figcaption></figcaption>
</div>

The main problem with the Monte Carlo algorithm is that many
state–action pairs may never be visited. With deterministic policies,
following *π* yields returns for only one action per state. Despite
employing exploring starts, insufficient training steps or inadequate
exploration may contribute to this outcome.

<div class="image-row" id="fig:task2_1-4">
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/bad_cliff/moving_avg_return.png"/>
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/bad_cliff/num_steps.png"/>
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/bad_cliff/num_steps_per_episode.png"/>
</div>
<div style="text-align: center;">
<figcaption></figcaption>
</div>

We observe that the exploring starts assumption proves occasionally
useful but cannot be generally relied upon. Starting conditions are
unlikely to provide such assistance. The alternative involves
considering only stochastic policies with non-zero probabilities for
selecting all actions in each state .  
We can also observe in Figure
<a href="#fig:mc_policy" data-reference-type="ref"
data-reference="fig:mc_policy">10</a> that the policy of the `CLIFF`
environment for the Monte Carlo algorithm after 25K iterations has not
converged whatsoever. This might indicate requiring more training
iterations. It is also worth noting that the cells in the policy map, in
Figure <a href="#fig:mc_policy" data-reference-type="ref"
data-reference="fig:mc_policy">10</a>, furthest from the begin state
generally have low probabilities and are relatively small. This might
indicate that the agent did not explore this region extensively.

<div class="image-grid" id="fig:task2_1-4">
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/good_monster/monte_carlo/episode.gif"
       style="width: auto; height: 50%;"/>
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/bad_cliff/monte_carlo/episode.gif"/>
</div>

Since the Monte Carlo method waits until the episode is completely over
before using the actual total reward to update the value estimates for
each visited state, they generally converge more slowly than
temporal-difference (TD) alternatives, as demonstrated in this example.

<div style="text-align: center;">
  <img src="/assets/images/2025-06-01/task2_1/mc_vs_td/bad_cliff/monte_carlo_policy.png">
</div>
<br>

#### different learning rates analysis

Once again, let’s start with a brief overview: The *α* coefficient
functions as a learning rate in TD methods. It determines how much new
information overrides old estimates. A higher *α* means the agent places
more weight on the most recent reward and bootstrapped value, adapting
quickly but risking instability. A lower *α* leads to slower updates,
making learning more stable but sluggish, potentially taking forever to
correct bad estimates. From the theory, if the step size *α* parameter
is reduced properly over time, this approach achieves convergence in any
stationary environment toward the actual action probabilities for each
state .

In this section, we will experiment with some different constant
learning coefficients (0.1, 0.2, 0.3, 0.4) within the `ROOM_WITH_LAVA`
environment. From Figure
<a href="#fig:different_learning_rates_policy" data-reference-type="ref"
data-reference="fig:different_learning_rates_policy">11</a>, it shows
distinct policy evolution and convergence characteristics across
different parameter settings.

<div style="text-align: center;">
    <div class="image-grid" id="different_learning_rates_policy">
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p1_policy.png"/>
        <figcaption>Learning rate = 0.1</figcaption>
      </figure>
      <figure>
      <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p2_policy.png"/>
        <figcaption>Learning rate = 0.2</figcaption>
      </figure>
      <figure>
      <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p3_policy.png"/>
        <figcaption>Learning rate = 0.3</figcaption>
      </figure>
      <figure>
      <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p4_policy.png"/>
        <figcaption>Learning rate = 0.4</figcaption>
      </figure>
    </div>
</div>
<figcaption></figcaption>
<br>

We observe exactly what was theoretically laid down. From Figure
<a href="#fig:different_learning_rates_plot" data-reference-type="ref"
data-reference="fig:different_learning_rates_plot">12</a>, we see that
small *α* coefficients convergence the slowest for the Q-learning
algorithm. Consequently, bigger *α* values in the Q-learning algorithm
results in faster convergence.

<div class="image-row" id="fig:different_learning_rates_plot">
  <img src="/assets/images/2025-06-01/task2_1/learning_rates/moving_avg_return.png"/>
  <img src="/assets/images/2025-06-01/task2_1/learning_rates/num_steps.png"/>
  <img src="/assets/images/2025-06-01/task2_1/learning_rates/num_steps_per_episode.png"/>
</div>

We also observe that the *α* value influences the agent’s explorability.
From Figure
<a href="#fig:different_learning_rates_visit" data-reference-type="ref"
data-reference="fig:different_learning_rates_visit">13</a>, the smaller
the learning rate, the more the agent explores, whereas a larger *α*
makes the agent more exploitative. This agrees with theory: a larger *α*
leads to faster convergence, leaving less time for exploration.

<figure id="fig:different_learning_rates_visit">
<div class="minipage">

</div>
<div class="minipage">

</div>
<figcaption></figcaption>
</figure>

<div style="text-align: center;">
    <div class="image-grid" id="different_learning_rates_visit">
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p1/state_visit_count_no_bg.png"/>
        <figcaption>Learning rate = 0.1</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p2/state_visit_count_no_bg.png"/>
        <figcaption>Learning rate = 0.2</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p3/state_visit_count_no_bg.png"/>
        <figcaption>Learning rate = 0.3</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/learning_rates/qlearning_0p4/state_visit_count_no_bg.png"/>
        <figcaption>Learning rate = 0.4</figcaption>
      </figure>
    </div>
</div>
<figcaption></figcaption>
<br>

<span id="sec:2.2.4" label="sec:different_epsilon"></span>
#### exploration-exploitation trade-off analysis

As usual, we start with some background information: Strategies that
incorporate exploration mechanisms tend to achieve superior long-term
performance due to their continued search for better solutions. As an
example, *ϵ*-greedy is such a mechanism that selects with probability
*ϵ* an action at random. Extremes of this interval are likely to result
in poor trajectories, as there would not be an appropriate trade-off
between exploration and exploitation during training.

<div style="text-align: center;">
    <div class="image-grid" id="fig:idc2w">
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/moving_avg_return.png"/>
        <!-- <figcaption>Learning rate = 0.1</figcaption> -->
      </figure>
      <figure>
      <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/num_steps.png"/>
        <!-- <figcaption>Learning rate = 0.2</figcaption> -->
      </figure>
    </div>
</div>
<br>

The experimental outcomes reveal a counter-intuitive finding: zero
exploration (*ϵ* = 0) produces the highest mean reward performance in
the `ROOM_WITH_MONSTER` environment. This result contradicts theoretical
expectations regarding exploration necessity. It is also the case that
the learned policy successfully accomplishes the intended objectives,
which is managing the ghouls encounters (through avoidance or combat)
while consistently reaching target locations. This can indicate that no
exploration was needed as the state-space was sufficiently small to find
the right optimal policy. We suspect that due to the environment’s small
state space and deterministic nature, such behavior is possible.
Interestingly, increasing the exploitation factor actually worsens
performance, at least within the episode cap of 25,000. However, we
expect that with enough episodes, all policies will eventually converge
to the optimal one.

<div style="text-align: center;">
    <div class="image-grid" id="different_learning_rates_policy">
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/qlearning_0p0/episode.gif"/>
        <figcaption>Exploration rate = 0.0</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/qlearning_0p25/episode.gif"/>
        <figcaption>Exploration rate = 0.25</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/qlearning_0p5/episode.gif"/>
        <figcaption>Exploration rate = 0.5</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/monster/qlearning_0p75/episode.gif"/>
        <figcaption>Exploration rate = 0.75</figcaption>
      </figure>
    </div>
</div>
<figcaption></figcaption>
<br>

When inspecting another environment, `CLIFF`, under the same conditions,
we find similar results.

<div style="text-align: center;">
    <div class="image-grid" id="idc2wea">
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/cliff/moving_avg_return.png"/>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_1/exploration_rates/cliff/num_steps.png"/>
      </figure>
    </div>
</div>
<figcaption></figcaption>
<br>

<span id="sec:2.2" label="sec:2.2"></span>
### 2.2 Linaer Exploration Rate

In previous 
<a href="#exploration-exploitation-trade-off-analysis">section</a>, we
experimented with different constant *ϵ* values. In this section, we
will use a linearly decreasing schedule, where the exploration parameter
diminishes progressively throughout the learning process. This approach
aims to balance initial exploration with eventual exploitation by
gradually reducing random action selection after each episode.
We will hold the learning rate at *α* = 0.3, the exploration schedule
will be a linear decay from 1.0 to 0.0 over 25, 000 steps (amount of
episodes). The discount factor will be 1.0 as well. The trained agent
was evaluated at multiple checkpoints
(1, 000, 2, 500, 5, 000, 7, 500, 10, 000) episodes to observe policy
evolution. For Q-learning, the target policy represents greedy action
selection, while Sarsa maintains *ϵ*-greedy behaviour during evaluation.

#### convergence behaviour analysis

The experimental results reveal an unexpected outcome: both Sarsa and
Q-learning algorithms converge to identical target policies. This
finding contradicts our earlier finding and expectations, particularly
given their fundamental differences in policy evaluation approaches.  
Several factors may contribute to this convergence pattern:

- The presence of exploring starts throughout experimentation  
  may have influenced the learning dynamics

- The selected evaluation intervals might be too coarse to capture  
  algorithmic differences with a relatively high *α* value.

- Fast convergence may obscure the expected behavioural distinctions

Detailed action probability distributions for each state across
different training checkpoints are provided as appendix, see Appendix
<a href="#sec:appendix_d" data-reference-type="ref"
data-reference="sec:appendix_d">[sec:appendix_d]</a>.  
In Figure <a href="#fig:woowze" data-reference-type="ref"
data-reference="fig:woowze">16</a>, as training approaches 25,000
episodes and exploration approaches zero, trajectory stochasticity
should diminish, leading to a consistent policy regardless of additional
training iterations. However, in this case, due to rapid convergence, we
didn’t observe any exploration because of the high *ϵ*-effect. We
believe this is caused by the high *α* value.

<div style="text-align: center;">
    <div class="image-grid" id="different_learning_rates_policy">
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp0/qlearning/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.0</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp0/sarsa/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.25</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp1/qlearning/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.5</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp1/sarsa/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.75</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp2/qlearning/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.75</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp2/sarsa/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.0</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp3/qlearning/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.25</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp3/sarsa/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.5</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp4/qlearning/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.75</figcaption> -->
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp4/sarsa/episode.gif"/>
        <!-- <figcaption>Exploration rate = 0.75</figcaption> -->
      </figure>
    </div>
</div>
<figcaption>
Target Policy after x training episodes. 
Left column is QLearning and right column is Sarsa.
The first row records a trajectory after training for a 1000 episodes,
the second row is the trajectory after training for 2500 iterations, etc.
</figcaption>
<br>

The complete implementation of the scheduling mechanism can be found in
the following file: `src/schedule.py`. The generation of the plots in
terms of the target policy can be found in:
`src/algorithms/qlearning:act` & `src/algorithms/sarsa:act` with both
`eval_target` and `is_evaluation` set to true.

### 2.3 Planning And Learning

A quick recap: by a model of the environment we mean anything that an
agent can use to predict how the environment will respond to its
actions. Given a state and an action, a model produces a prediction of
the resultant next state and next reward .  
The DynaQ implementation utilizes table-based model learning under
deterministic world assumptions, returning previously observed
state-reward pairs when queried with familiar state-action combinations.

#### planning horizon impact analysis

Despite all agents achieving optimal performance levels, the Dyna
Q-learning variant with 50 planning steps demonstrates the most ideal
symmetrical policy structure. This agent successfully recognizes that
paths within the top-left 4×4 grid region are all equivalent. Even
minimal planning (5 steps) outperforms standard Q-learning by
eliminating wall-directed actions in outer grid sections.

<div style="text-align: center;">
    <div class="image-grid" id="fig:woowze">
      <figure>
        <img src="/assets/images/2025-06-01/task2_3/empty_room/qlearning_policy.png"/>
        <figcaption>0 planning steps</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_3/empty_room/dyna_qlearning_5_policy.png"/>
        <figcaption>5 planning steps</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_3/empty_room/dyna_qlearning_20_policy.png"/>
        <figcaption>20 planning steps</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_3/empty_room/dyna_qlearning_50_policy.png"/>
        <figcaption>50 planning steps</figcaption>
      </figure>
    </div>
</div>

Moreover, in Figure
<a href="#fig:model_free_vs_Model_based_steps" data-reference-type="ref"
data-reference="fig:model_free_vs_Model_based_steps">17</a>, we observe
that planning agents achieve convergence considerably faster than
non-planning counterparts, with convergence speed proportional to
planning step quantity. This observation aligns with theoretical
predictions regarding planning agent efficiency.

<div style="text-align: center;">
    <img src="/assets/images/2025-06-01/task2_3/empty_room/num_steps.png"/>
</div>
<br>

The same observations are also present in the `CLIFF` environment.
See <a href="#sec:appendix_c" data-reference-type="ref"
data-reference="sec:appendix_c">Appendix</a> in order to inspect
those figures.

## 3. Deep Reinforcement Learning Implementation

In this section, we will implement a Deep Reinforcement Learning agent
on two environments `EMPTY_ROOM` and `ROOM_WITH_MULTIPLE_MONSTERS`,
which is an extension of the previous environment with multiple random
monsters spawning. Concretely, we will implement a deep Q-learning agent
using  
`stable_baselines3.DQN` and an actor critic method using
`stable_baselines3.PPO`.  
The ASCII-based representation will still be used as the state encoding
for these deep learning agents. However, the neural networks of the DRL
algorithms were configured with an input dimension of 64 nodes,
requiring a transformation mechanism to map the two-dimensional ASCII
representation into this compatible input space.  
A Convolutional Neural Network (CNN) was used to transform the
two-dimensional ASCII image representation into a one-dimensional vector
embedding. The network architecture begins with a convolutional layer
featuring a single input channel, eight output channels, and a 3x3
kernel with unit stride and padding. This is followed by ReLU activation
and 2x2 max pooling with stride 2. A second convolutional layer
processes eight input channels to produce sixteen output channels using
identical kernel and padding configurations, followed by another ReLU
activation and flattening operation.  
Afterwards, the flattened features are processed through a multi-layer
perceptron consisting of two fully connected layers with ReLU
activations. The trimmed ASCII observation data flows through the
convolutional layers, and the resulting output feeds into the fully
connected network to produce a 64-element one-dimensional vector
representation. The complete implementation of this feature extractor
can be found in the following file: `src/algorithms/cnn.py`.

### hyperparameter configuration

The learning rate was established at 0.0001, which differs fundamentally
from the *α* parameter used in tabular Q-learning and Sarsa
implementations. This learning rate governs the stochastic gradient
descent optimizer responsible for optimizing network weights. The
discount factor *γ* was set to 0.99, maintaining consistency with
temporal difference and Monte Carlo algorithm implementations. These
configuration parameters were applied uniformly across both PPO and DQN
implementations. For PPO, we increased entropy to 0.01 to encourage
exploration during training.

<div style="text-align: center;">
    <div class="image-grid" id="fig:woowziae">
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp1/moving_avg_return.png"/>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp3/moving_avg_return.png"/>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp5/moving_avg_return.png"/>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp7/moving_avg_return.png"/>
      </figure>
    </div>
</div>
<figcaption></figcaption>
<br>

### Tabular Q vs DQN

First, convergence speed represents the most notable difference, with
tabular methods achieving convergence substantially faster due to the
limited state-action space in simple grid environments. Tabular
approaches typically require only thousands of episodes for convergence,
while neural network training demands hundreds of thousands to millions
of episodes.  
The ultimate objective involves teaching agents to recognize goal
symbols regardless of their spatial placement within the environment.
Tabular Q-learning lacks this generalization capability, relying instead
on memorized state-action mappings from training experiences. This
limitation renders tabular methods ineffective for out-of-distribution
scenarios, a constraint not present in deep reinforcement learning
approaches.  
For example, once the DRL is trained on a 5 × 5 grid, it might be
possible to infer an action on a board of dimensions 6 × 6, which would
be impossible for the tabular methods.

### PPO vs DQN

Proximal Policy Optimization demonstrates superior characteristics in
terms of training speed, stability compared to DQN. However, policy
gradient approaches face inherent challenges related to premature
deterministic behaviour. When agents become overly deterministic early
in training, exploration ceases, potentially trapping the learning
process in local optima and preventing discovery of superior
strategies.  

<div style="text-align: center;">
    <div class="image-grid" id="fig:woowziae">
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp1/dqn/episode.gif"/>
        <figcaption>DQN</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp3/episode.gif"/>
        <figcaption>DQN</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp5/ppo/episode.gif"/>
        <figcaption>PPO</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task3_1/exp7/episode.gif"/>
        <figcaption>PPO</figcaption>
      </figure>
    </div>
</div>

## Appendix
<!-- <span id="sec:appendix_a" label="sec:appendix_a"></span> -->
<!--
### Appendix A: Policy Frames

<div style="text-align: center;">
    <div class="image-grid" id="fig:woowziae">
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp0/qlearning_policy.png"/>
        <figcaption>Qlearning - 1000 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp0/sarsa_policy.png"/>
        <figcaption>Sarsa - 1000 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp1/qlearning_policy.png"/>
        <figcaption>Qlearning - 2500 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp1/sarsa_policy.png"/>
        <figcaption>Sarsa - 2500 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp2/qlearning_policy.png"/>
        <figcaption>Qlearning - 5000 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp2/sarsa_policy.png"/>
        <figcaption>Sarsa - 5000 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp3/qlearning_policy.png"/>
        <figcaption>Qlearning - 7500 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp3/sarsa_policy.png"/>
        <figcaption>Sarsa - 7500 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp4/qlearning_policy.png"/>
        <figcaption>Qlearning - 10 000 episodes</figcaption>
      </figure>
      <figure>
        <img src="/assets/images/2025-06-01/task2_2/exp4/sarsa_policy.png"/>
        <figcaption>Sarsa - 10 000 episodes</figcaption>
      </figure>
    </div>
</div>

<span id="sec:appendix_b" label="sec:appendix_b"></span>

### Appendix B: Sarsa Policy Frames


<span id="sec:appendix_c" label="sec:appendix_c"></span>

### Appendix C: DynaQ In Cliff

id="fig:model_based_vs_model_free_cliff"

### Appendix D: Linear Exploration Rate

-->

## References

[^1]: [NetHack guidebook](https://www.nethack.org/v363/Guidebook.html)
